<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI in Audiology: Pupillometry Approaches - AI for HearTech</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <nav>
        <div class="container">
            <a href="index.html" class="logo">AI for HearTech</a>
            <ul class="nav-links">
                <li><a href="index.html#about">About</a></li>
                <li><a href="index.html#schedule">Schedule</a></li>
                <li><a href="index.html#resources">Resources</a></li>
                <li><a href="index.html#contact">Contact</a></li>
            </ul>
        </div>
    </nav>

    <header>
        <div class="container">
            <h1>AI in Audiology: Pupillometry Approaches for Objective Hearing Assessment</h1>
            <p>February 3, 2025 - Presented by Chuhui Liu (Leo)</p>
        </div>
    </header>
    
    <div class="container">
        <div class="session-details">
            <div><strong>Date:</strong> February 3, 2025</div>
            <div><strong>Presenter:</strong> Chuhui Liu (Leo)</div>
            <div><strong>Affiliation:</strong> PhD Student, Computer Science, University at Buffalo</div>
        </div>
        
        <div class="session-container">
            <div class="session-section">
                <h2>Session Overview</h2>
                <p>The first AI for HearTech Workshop of 2025 featured a presentation by PhD candidate Chuhui Liu (Leo) on AudioSight, a smart pupillometry system for objective hearing assessment. This session marked the beginning of our interdisciplinary seminar series exploring artificial intelligence applications in audiology and hearing sciences.</p>
                
                <div class="figure">
                    <img src="images/audiosight_header.jpg" alt="AudioSight System Overview">
                    <figcaption>AudioSight: Smart Pupillometry System for Hearing Assessment</figcaption>
                </div>
            </div>
            
            <div class="session-section">
                <h2>Smart Pupillometry for Audiology Assessment</h2>
                <p>Leo presented AudioSight, a novel approach using pupillometry to objectively assess hearing disorders. Traditional audiological assessments typically require active participation from patients, whereas pupillometry measures involuntary physiological responses to auditory stimuli.</p>
                
                <div class="figure">
                    <img src="images/pupillometry_devices.png" alt="AudioSight Device Prototypes">
                    <figcaption>AudioSight Device Prototypes: Desk Stand, Wearable, and Mobile Attachment</figcaption>
                </div>
                
                <p>Key aspects of the technology include:</p>
                <ul>
                    <li>Pupil dilation correlates with cognitive effort during auditory processing</li>
                    <li>The involuntary nature of pupil responses provides objective measurements</li>
                    <li>The system can potentially assess hearing function without requiring behavioral responses</li>
                    <li>Multiple hardware implementations (desk-mounted, wearable, and smartphone-attached versions) offer flexibility for different research and clinical contexts</li>
                </ul>
            </div>
            
            <div class="session-section">
                <h2>Technical Implementation</h2>
                <p>The AudioSight system integrates hardware with AI-driven software components:</p>
                
                <div class="figure">
                    <img src="images/ai_system_overview.png" alt="AI System Architecture">
                    <figcaption>AI-powered System Architecture for AudioSight</figcaption>
                </div>
                
                <ul>
                    <li>Object detection algorithms for pupil region identification</li>
                    <li>SAM V2 (Segment Anything Model) for precise pupil segmentation</li>
                    <li>LSTM (Long Short-Term Memory) networks for blink artifact rejection</li>
                    <li>Feature extraction from pupillary response functions</li>
                    <li>Classification models for identifying specific hearing disorders</li>
                </ul>
                
                <p>The team has developed specialized protocols to account for individual variability, including pupil muscular tests, hearing loudness tests with incremental increases, hearing effort tests with progressive noise addition, and sound aversion tests.</p>
                
                <div class="figure">
                    <img src="images/individual_variability_tests.png" alt="Test Protocols">
                    <figcaption>Test Protocols Addressing Individual Variability</figcaption>
                </div>
            </div>
            
            <div class="session-section">
                <h2>Preliminary Findings</h2>
                <p>Preliminary clinical trials included participants with hyperacusis, misophonia, tinnitus, and healthy controls. The system demonstrated promising accuracy in differentiating these conditions, though challenges remain in cases where participants present with multiple disorders.</p>
                
                <div class="figure">
                    <img src="images/hearing_disorder_results.png" alt="Classification Results">
                    <figcaption>Classification Accuracy Results for Hearing Disorders</figcaption>
                </div>
                
                <p>The confusion matrices show particularly strong performance for hyperacusis (96.15%) and tinnitus (96.15%) detection, with room for improvement in misophonia classification (75%).</p>
            </div>
            
            <div class="session-section">
                <h2>Research Discussion</h2>
                <p>The presentation generated substantive discussion around several key themes:</p>
                <ul>
                    <li>Methodological approaches to separating and quantifying multiple co-occurring hearing conditions</li>
                    <li>Potential integration with established clinical measures like loudness discomfort level tests and tinnitus pitch matching</li>
                    <li>The dual potential of AI in audiology: automating established processes and enabling innovation in assessment procedures</li>
                    <li>Limitations of traditional labeled data approaches and the potential for reinforcement learning and foundation models</li>
                </ul>
            </div>
            
            <div class="session-section">
                <h2>Next Steps</h2>
                <ul>
                    <li>Elizabeth or another student from her lab to present on an audiology-related topic at the next meeting on February 17th</li>
                    <li>Leo to consider incorporating additional hearing tests (e.g., loudness discomfort levels, pitch matching) into the pupillometry study for comparison</li>
                    <li>Research team to brainstorm new testing procedures that go beyond traditional audiometry methods</li>
                    <li>Research team to investigate the potential application of reinforcement learning in the hearing tech research</li>
                    <li>Research team to explore ways to use AGI for developing new clinical procedures in hearing tests</li>
                </ul>
            </div>
            
            <div class="session-section">
                <h2>Future Directions</h2>
                <p>The discussion highlighted several promising research directions:</p>
                <ul>
                    <li>Incorporating additional clinical tests for comprehensive assessment</li>
                    <li>Developing regression models for quantifying symptom severity</li>
                    <li>Transitioning from pure tone stimuli to more complex phonetic signals</li>
                    <li>Exploring AGI (Artificial General Intelligence) approaches that reduce dependence on labeled data</li>
                </ul>
                
                <p>Our next session is scheduled for February 17th, where we will continue exploring the intersection of AI and audiology with another research presentation.</p>
            </div>
        </div>
        
        <div class="session-card">
            <div class="session-header">
                Next Session: February 17, 2025
            </div>
            <div class="session-content">
                <div class="session-meta">
                    <span><strong>Presenter:</strong> Rania A. Sharaf, MSc</span>
                    <span><strong>Affiliation:</strong> Clinical Audiologist, Ph.D. Student, Communication Disorders and Sciences, University at Buffalo</span>
                </div>
                <p>Join us for the next session on clinical applications of AI in audiology.</p>
                <a href="index.html#schedule">View Full Schedule</a>
            </div>
        </div>
    </div>
    
    <footer>
        <div class="container">
            <p>&copy; 2025 AI for HearTech Seminar Series. All rights reserved.</p>
            <p>University at Buffalo</p>
        </div>
    </footer>
</body>
</html>
